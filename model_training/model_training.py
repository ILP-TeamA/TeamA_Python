"""
å®Œæ•´çš„å•¤é…’é¢„æµ‹æ¨¡å‹è®­ç»ƒå™¨ - è¶…å‚æ•°ä¼˜åŒ–ç‰ˆ
ä½¿ç”¨äº¤å‰éªŒè¯è‡ªåŠ¨è°ƒä¼˜æœ€ä½³è¶…å‚æ•°ï¼Œæé«˜æ¨¡å‹ç²¾åº¦
"""

import pandas as pd
import numpy as np
import pickle
import warnings
import os
from datetime import datetime
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
from scipy.stats import randint, uniform
import time

warnings.filterwarnings('ignore')

class HyperOptBeerModelTrainer:
    """è¶…å‚æ•°ä¼˜åŒ–å•¤é…’æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, use_randomized_search=True, cv_folds=3, n_iter=50):
        """
        åˆå§‹åŒ–è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒå™¨
        
        Args:
            use_randomized_search: æ˜¯å¦ä½¿ç”¨éšæœºæœç´¢ï¼ˆæ›´å¿«ï¼‰ï¼ŒFalseåˆ™ä½¿ç”¨ç½‘æ ¼æœç´¢ï¼ˆæ›´ç²¾ç¡®ï¼‰
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            n_iter: éšæœºæœç´¢æ—¶çš„è¿­ä»£æ¬¡æ•°
        """
        self.beer_types = ['ãƒšãƒ¼ãƒ«ã‚¨ãƒ¼ãƒ«', 'ãƒ©ã‚¬ãƒ¼', 'IPA', 'ãƒ›ãƒ¯ã‚¤ãƒˆãƒ“ãƒ¼ãƒ«', 'é»’ãƒ“ãƒ¼ãƒ«', 'ãƒ•ãƒ«ãƒ¼ãƒ„ãƒ“ãƒ¼ãƒ«']
        self.models = {}
        self.best_params = {}
        self.scalers = {}
        self.feature_names = []
        self.stats = {}
        self.use_randomized_search = use_randomized_search
        self.cv_folds = cv_folds
        self.n_iter = n_iter
        
    def load_and_prepare_data(self, sales_file='sales.csv', weather_file='weather.csv'):
        """æ•°æ®åŠ è½½å’Œé¢„å¤„ç†"""
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        
        try:
            sales_df = pd.read_csv(sales_file, encoding='utf-8')
            weather_df = pd.read_csv(weather_file, encoding='utf-8')
            
            print(f"âœ… é”€å”®æ•°æ®: {len(sales_df)}è¡Œ x {len(sales_df.columns)}åˆ—")
            print(f"âœ… å¤©æ°”æ•°æ®: {len(weather_df)}è¡Œ x {len(weather_df.columns)}åˆ—")
            
            # æ—¥æœŸå¤„ç†
            sales_df['date'] = pd.to_datetime(sales_df['æ—¥ä»˜'])
            weather_df['date'] = pd.to_datetime(weather_df['date'])
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.merge(sales_df, weather_df, on='date', how='inner')
            merged_df = merged_df.sort_values('date').reset_index(drop=True)
            
            print(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ: {len(merged_df)}è¡Œ")
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            self._analyze_data_quality(merged_df)
            
            return merged_df
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _analyze_data_quality(self, df):
        """æ•°æ®è´¨é‡åˆ†æ"""
        print("\nğŸ” æ•°æ®è´¨é‡åˆ†æ:")
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {df['date'].min()} ~ {df['date'].max()}")
        print(f"ğŸ“Š æ€»å¤©æ•°: {len(df)}å¤©")
        
        # æ¥å®¢æ•°ä¸é”€é‡å…³ç³»åˆ†æ
        beer_cols = [col for col in df.columns if col.endswith('(æœ¬)')]
        if beer_cols and 'æ¥å®¢æ•°' in df.columns:
            df['total_beer_sales'] = df[beer_cols].sum(axis=1)
            correlation = df['æ¥å®¢æ•°'].corr(df['total_beer_sales'])
            avg_ratio = (df['total_beer_sales'] / df['æ¥å®¢æ•°']).mean()
            
            print(f"ğŸº æ¥å®¢æ•°ä¸æ€»é”€é‡ç›¸å…³æ€§: {correlation:.3f}")
            print(f"ğŸ‘¥ å¹³å‡äººå‡æ¶ˆè´¹: {avg_ratio:.2f}æ¯/äºº")
            
            if correlation < 0.5:
                print(f"   âš ï¸ ç›¸å…³æ€§è¾ƒä½ï¼éœ€è¦å¼ºåŒ–ç‰¹å¾å·¥ç¨‹")
            else:
                print(f"   âœ… ç›¸å…³æ€§è‰¯å¥½")
        
        # ç¼ºå¤±å€¼æ£€æŸ¥
        missing_values = df.isnull().sum()
        if missing_values.any():
            print("\nâš ï¸ ç¼ºå¤±å€¼:")
            for col, count in missing_values[missing_values > 0].items():
                print(f"  {col}: {count}ä¸ª ({count/len(df)*100:.1f}%)")
        else:
            print("\nâœ… æ— ç¼ºå¤±å€¼")
    
    def create_improved_features(self, df):
        """æ”¹è¿›çš„ç‰¹å¾å·¥ç¨‹"""
        print("\nğŸš€ æ”¹è¿›ç‰¹å¾å·¥ç¨‹ - å¼ºåŒ–æ¥å®¢æ•°å…³ç³»")
        feature_df = df.copy()
        
        # ğŸ”¥ ç¬¬1ä¼˜å…ˆçº§ï¼šå¼ºåŒ–æ¥å®¢æ•°ç‰¹å¾
        print("  ğŸ‘¥ å¼ºåŒ–æ¥å®¢æ•°ç‰¹å¾ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰...")
        if 'æ¥å®¢æ•°' in feature_df.columns:
            # 1. æ¥å®¢æ•°æ ¸å¿ƒç‰¹å¾
            feature_df['customers'] = feature_df['æ¥å®¢æ•°']
            feature_df['customers_squared'] = feature_df['æ¥å®¢æ•°'] ** 2
            feature_df['customers_log'] = np.log1p(feature_df['æ¥å®¢æ•°'])
            feature_df['customers_sqrt'] = np.sqrt(feature_df['æ¥å®¢æ•°'])
            
            # 2. æ¥å®¢æ•°åˆ†çº§ç‰¹å¾
            feature_df['customers_low'] = (feature_df['æ¥å®¢æ•°'] < 15).astype(int)
            feature_df['customers_medium'] = ((feature_df['æ¥å®¢æ•°'] >= 15) & (feature_df['æ¥å®¢æ•°'] < 25)).astype(int)
            feature_df['customers_high'] = (feature_df['æ¥å®¢æ•°'] >= 25).astype(int)
            
            # 3. æ¥å®¢æ•°æ ‡å‡†åŒ–
            customers_mean = feature_df['æ¥å®¢æ•°'].mean()
            customers_std = feature_df['æ¥å®¢æ•°'].std()
            if customers_std > 0:
                feature_df['customers_normalized'] = (feature_df['æ¥å®¢æ•°'] - customers_mean) / customers_std
            
            print(f"    âœ… æ¥å®¢æ•°åŸºç¡€ç‰¹å¾: 8ä¸ª")
        
        # ğŸ”¥ ç¬¬2ä¼˜å…ˆçº§ï¼šæ ¸å¿ƒæ—¶é—´ç‰¹å¾
        print("  ğŸ“… æ ¸å¿ƒæ—¶é—´ç‰¹å¾...")
        feature_df['day_of_week'] = feature_df['date'].dt.dayofweek + 1
        feature_df['is_weekend'] = (feature_df['day_of_week'].isin([6, 7])).astype(int)
        feature_df['is_friday'] = (feature_df['day_of_week'] == 5).astype(int)
        feature_df['month'] = feature_df['date'].dt.month
        
        # å­£èŠ‚æ€§ç¼–ç 
        feature_df['month_sin'] = np.sin(2 * np.pi * feature_df['month'] / 12)
        feature_df['month_cos'] = np.cos(2 * np.pi * feature_df['month'] / 12)
        feature_df['day_sin'] = np.sin(2 * np.pi * feature_df['day_of_week'] / 7)
        feature_df['day_cos'] = np.cos(2 * np.pi * feature_df['day_of_week'] / 7)
        
        print(f"    âœ… æ—¶é—´ç‰¹å¾: 8ä¸ª")
        
        # ğŸ”¥ ç¬¬3ä¼˜å…ˆçº§ï¼šå…³é”®å¤©æ°”ç‰¹å¾
        print("  ğŸŒ¤ï¸ å…³é”®å¤©æ°”ç‰¹å¾...")
        weather_features = ['avg_temperature', 'total_rainfall', 'sunshine_hours']
        
        for feature in weather_features:
            if feature in feature_df.columns:
                # æ ‡å‡†åŒ–
                mean_val = feature_df[feature].mean()
                std_val = feature_df[feature].std()
                if std_val > 0:
                    feature_df[f'{feature}_norm'] = (feature_df[feature] - mean_val) / std_val
                    # æ·»åŠ éçº¿æ€§å˜æ¢
                    feature_df[f'{feature}_squared'] = feature_df[feature] ** 2
                else:
                    feature_df[f'{feature}_norm'] = 0
                    feature_df[f'{feature}_squared'] = feature_df[feature]
        
        # å¤©æ°”èˆ’é€‚åº¦æŒ‡æ•°
        if all(f'{f}_norm' in feature_df.columns for f in weather_features):
            feature_df['weather_comfort'] = (
                (1 - np.abs(feature_df['avg_temperature_norm'])) * 
                (1 - np.abs(feature_df['total_rainfall_norm'])) *
                (1 + feature_df['sunshine_hours_norm'])
            )
        
        # æ¸©åº¦åˆ†çº§
        if 'avg_temperature' in feature_df.columns:
            feature_df['temp_cold'] = (feature_df['avg_temperature'] < 10).astype(int)
            feature_df['temp_cool'] = ((feature_df['avg_temperature'] >= 10) & (feature_df['avg_temperature'] < 20)).astype(int)
            feature_df['temp_warm'] = ((feature_df['avg_temperature'] >= 20) & (feature_df['avg_temperature'] < 30)).astype(int)
            feature_df['temp_hot'] = (feature_df['avg_temperature'] >= 30).astype(int)
        
        print(f"    âœ… å¤©æ°”ç‰¹å¾: 10ä¸ª")
        
        # ğŸ”¥ ç¬¬4ä¼˜å…ˆçº§ï¼šæ¥å®¢æ•°äº¤äº’ç‰¹å¾
        print("  ğŸ¤ æ¥å®¢æ•°äº¤äº’ç‰¹å¾...")
        if 'æ¥å®¢æ•°' in feature_df.columns:
            # æ¥å®¢æ•° Ã— å¤©æ°”
            if 'avg_temperature_norm' in feature_df.columns:
                feature_df['customers_temp_interaction'] = (
                    feature_df['æ¥å®¢æ•°'] * feature_df['avg_temperature_norm']
                )
            
            # æ¥å®¢æ•° Ã— å‘¨æœ«
            feature_df['customers_weekend_interaction'] = (
                feature_df['æ¥å®¢æ•°'] * feature_df['is_weekend']
            )
            
            # æ¥å®¢æ•° Ã— å¤©æ°”èˆ’é€‚åº¦
            if 'weather_comfort' in feature_df.columns:
                feature_df['customers_comfort_interaction'] = (
                    feature_df['æ¥å®¢æ•°'] * feature_df['weather_comfort']
                )
            
            # æ¥å®¢æ•° Ã— æœˆä»½
            feature_df['customers_month_interaction'] = (
                feature_df['æ¥å®¢æ•°'] * feature_df['month']
            )
        
        print(f"    âœ… äº¤äº’ç‰¹å¾: 4ä¸ª")
        
        # ğŸ”¥ ç¬¬5ä¼˜å…ˆçº§ï¼šç²¾ç®€æ»åç‰¹å¾
        print("  ğŸ“ˆ ç²¾ç®€æ»åç‰¹å¾...")
        
        lag_features_count = 0
        for beer in self.beer_types:
            beer_col = f'{beer}(æœ¬)'
            if beer_col not in feature_df.columns:
                continue
            
            # åªä¿ç•™æœ€é‡è¦çš„æ»åç‰¹å¾
            feature_df[f'{beer}_lag1'] = feature_df[beer_col].shift(1)
            feature_df[f'{beer}_ma3'] = feature_df[beer_col].rolling(3, min_periods=1).mean()
            feature_df[f'{beer}_ma7'] = feature_df[beer_col].rolling(7, min_periods=1).mean()
            
            # å®¢å•ä»·ç‰¹å¾
            if 'æ¥å®¢æ•°' in feature_df.columns:
                historical_ratio = feature_df[beer_col] / feature_df['æ¥å®¢æ•°'].replace(0, 1)
                feature_df[f'{beer}_customer_ratio_ma3'] = historical_ratio.rolling(3, min_periods=1).mean()
            
            lag_features_count += 4
        
        # æ¥å®¢æ•°æ»å
        if 'æ¥å®¢æ•°' in feature_df.columns:
            feature_df['customers_lag1'] = feature_df['æ¥å®¢æ•°'].shift(1)
            feature_df['customers_ma3'] = feature_df['æ¥å®¢æ•°'].rolling(3, min_periods=1).mean()
            feature_df['customers_ma7'] = feature_df['æ¥å®¢æ•°'].rolling(7, min_periods=1).mean()
            lag_features_count += 3
        
        print(f"    âœ… æ»åç‰¹å¾: {lag_features_count}ä¸ª")
        
        # å¤„ç†ç¼ºå¤±å€¼
        feature_df = feature_df.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        total_features = len([col for col in feature_df.columns 
                             if col not in ['date', 'æ—¥ä»˜', 'æ›œæ—¥', 'å¹´', 'æœˆ', 'æ—¥', 'å­£ç¯€']])
        print(f"\nâœ… æ”¹è¿›ç‰¹å¾å·¥ç¨‹å®Œæˆ: æ€»è®¡çº¦{total_features}ä¸ªç‰¹å¾")
        
        return feature_df
    
    def select_optimal_features(self, feature_df):
        """é€‰æ‹©æœ€ä¼˜ç‰¹å¾é›†"""
        print("\nğŸ¯ é€‰æ‹©æœ€ä¼˜ç‰¹å¾é›†...")
        
        # æ’é™¤ç›®æ ‡å˜é‡å’Œéç‰¹å¾åˆ—
        exclude_cols = [
            'date', 'æ—¥ä»˜', 'æ›œæ—¥', 'å£²ä¸Šåˆè¨ˆ(å††)', 'å¹´', 'æœˆ', 'æ—¥', 'å­£ç¯€', 
            'total_cup', 'ç·æ¯æ•°'
        ]
        target_patterns = ['(æœ¬)', '(å††)']
        
        all_cols = feature_df.columns
        feature_cols = [col for col in all_cols 
                       if col not in exclude_cols and 
                       not any(pattern in col for pattern in target_patterns)]
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
        priority_groups = {
            'æ¥å®¢æ•°æ ¸å¿ƒ': [col for col in feature_cols if 'customers' in col and 'lag' not in col and 'ma' not in col and 'interaction' not in col],
            'æ—¶é—´ç‰¹å¾': [col for col in feature_cols if any(t in col for t in ['day_of_week', 'is_weekend', 'is_friday', 'month', '_sin', '_cos'])],
            'å¤©æ°”ç‰¹å¾': [col for col in feature_cols if any(w in col for w in ['temperature', 'rainfall', 'sunshine', 'weather_comfort', 'temp_'])],
            'äº¤äº’ç‰¹å¾': [col for col in feature_cols if 'interaction' in col],
            'æ»åç‰¹å¾': [col for col in feature_cols if any(s in col for s in ['lag', 'ma3', 'ma7', 'ratio_ma3'])]
        }
        
        # æ„å»ºæœ€ç»ˆç‰¹å¾é›†
        final_features = []
        for group_name, group_features in priority_groups.items():
            group_features = [f for f in group_features if f in feature_cols]
            final_features.extend(group_features)
            print(f"  {group_name}: {len(group_features)}ä¸ª")
        
        # å»é‡
        final_features = list(dict.fromkeys(final_features))
        
        # æ£€æŸ¥ç‰¹å¾/æ ·æœ¬æ¯”ä¾‹
        ratio = len(final_features) / len(feature_df)
        print(f"\nğŸ“Š æœ€ç»ˆç‰¹å¾ç»Ÿè®¡:")
        print(f"  æ€»ç‰¹å¾æ•°: {len(final_features)}ä¸ª")
        print(f"  æ ·æœ¬æ•°: {len(feature_df)}ä¸ª")
        print(f"  ç‰¹å¾/æ ·æœ¬æ¯”ä¾‹: {ratio:.3f}")
        
        if ratio < 0.1:
            print("  âœ… æ¯”ä¾‹ä¼˜ç§€ï¼è¿‡æ‹Ÿåˆé£é™©å¾ˆä½")
        elif ratio < 0.15:
            print("  ğŸ‘ æ¯”ä¾‹è‰¯å¥½ï¼")
        else:
            print("  âš ï¸ æ¯”ä¾‹åé«˜ï¼Œä½†å¯ä»¥æ¥å—")
        
        self.feature_names = final_features
        return final_features
    
    def get_hyperparameter_spaces(self):
        """å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´"""
        if self.use_randomized_search:
            # éšæœºæœç´¢å‚æ•°ç©ºé—´
            param_spaces = {
                'RandomForest': {
                    'n_estimators': randint(100, 300),
                    'max_depth': randint(8, 20),
                    'min_samples_split': randint(2, 10),
                    'min_samples_leaf': randint(1, 5),
                    'max_features': ['sqrt', 'log2', 0.8, 0.9],
                    'bootstrap': [True, False]
                },
                'GradientBoosting': {
                    'n_estimators': randint(80, 200),
                    'max_depth': randint(4, 10),
                    'learning_rate': uniform(0.05, 0.15),
                    'min_samples_split': randint(2, 8),
                    'min_samples_leaf': randint(1, 4),
                    'subsample': uniform(0.8, 0.2)
                },
                'ExtraTrees': {
                    'n_estimators': randint(100, 300),
                    'max_depth': randint(8, 20),
                    'min_samples_split': randint(2, 10),
                    'min_samples_leaf': randint(1, 5),
                    'max_features': ['sqrt', 'log2', 0.8, 0.9],
                    'bootstrap': [True, False]
                },
                # 'Ridge': {
                #     'alpha': uniform(0.1, 10),
                #     'fit_intercept': [True, False],
                #     'solver': ['auto', 'svd', 'cholesky', 'lsqr']
                # },
                # 'Lasso': {
                #     'alpha': uniform(0.1, 10),
                #     'fit_intercept': [True, False],
                #     'max_iter': [1000, 2000, 3000]
                # },
                # 'ElasticNet': {
                #     'alpha': uniform(0.1, 5),
                #     'l1_ratio': uniform(0.1, 0.8),
                #     'fit_intercept': [True, False],
                #     'max_iter': [1000, 2000, 3000]
                # }
            }
        else:
            # ç½‘æ ¼æœç´¢å‚æ•°ç©ºé—´ï¼ˆæ›´ç²¾ç¡®ä½†æ›´æ…¢ï¼‰
            param_spaces = {
                'RandomForest': {
                    'n_estimators': [100, 150, 200, 250],
                    'max_depth': [10, 12, 15, 18],
                    'min_samples_split': [2, 5, 8],
                    'min_samples_leaf': [1, 2, 3],
                    'max_features': ['sqrt', 'log2', 0.8]
                },
                'GradientBoosting': {
                    'n_estimators': [80, 100, 120, 150],
                    'max_depth': [4, 6, 8],
                    'learning_rate': [0.05, 0.1, 0.15],
                    'min_samples_split': [2, 5],
                    'min_samples_leaf': [1, 2]
                },
                'ExtraTrees': {
                    'n_estimators': [100, 150, 200],
                    'max_depth': [10, 15, 18],
                    'min_samples_split': [2, 5],
                    'min_samples_leaf': [1, 2],
                    'max_features': ['sqrt', 0.8]
                },
                # 'Ridge': {
                #     'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],
                #     'fit_intercept': [True, False]
                # },
                # 'Lasso': {
                #     'alpha': [0.1, 0.5, 1.0, 2.0],
                #     'fit_intercept': [True, False]
                # },
                # 'ElasticNet': {
                #     'alpha': [0.1, 0.5, 1.0, 2.0],
                #     'l1_ratio': [0.2, 0.5, 0.8],
                #     'fit_intercept': [True, False]
                # }
            }
        
        return param_spaces
    
    def hyperparameter_optimization(self, beer_name, X, y):
        """è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒ"""
        print(f"\nğŸ¯ {beer_name} è¶…å‚æ•°ä¼˜åŒ–...")
        
        # æ—¶é—´åºåˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=self.cv_folds)
        
        # æ¨¡å‹é…ç½®
        models_config = {
            'RandomForest': {
                'model': RandomForestRegressor(random_state=42, n_jobs=-1),
                'use_scaling': False
            },
            'GradientBoosting': {
                'model': GradientBoostingRegressor(random_state=42),
                'use_scaling': False
            },
            'ExtraTrees': {
                'model': ExtraTreesRegressor(random_state=42, n_jobs=-1),
                'use_scaling': False
            },
            # 'Ridge': {
            #     'model': Ridge(),
            #     'use_scaling': True
            # },
            # 'Lasso': {
            #     'model': Lasso(),
            #     'use_scaling': True
            # },
            # 'ElasticNet': {
            #     'model': ElasticNet(),
            #     'use_scaling': True
            # }
        }
        
        param_spaces = self.get_hyperparameter_spaces()
        
        best_model = None
        best_score = -float('inf')
        best_name = ''
        best_params = {}
        
        for name, config in models_config.items():
            if name not in param_spaces:
                continue
                
            print(f"  ğŸ” ä¼˜åŒ– {name}...")
            start_time = time.time()
            
            try:
                model = config['model']
                use_scaling = config['use_scaling']
                param_space = param_spaces[name]
                
                # é€‰æ‹©æœç´¢æ–¹æ³•
                if self.use_randomized_search:
                    search = RandomizedSearchCV(
                        model, param_space, 
                        n_iter=self.n_iter,
                        cv=tscv, 
                        scoring='r2',
                        n_jobs=-1 if name in ['RandomForest', 'ExtraTrees'] else 1,
                        random_state=42,
                        verbose=0
                    )
                else:
                    search = GridSearchCV(
                        model, param_space,
                        cv=tscv,
                        scoring='r2',
                        n_jobs=-1 if name in ['RandomForest', 'ExtraTrees'] else 1,
                        verbose=0
                    )
                
                # å‡†å¤‡æ•°æ®
                if use_scaling:
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                    search.fit(X_scaled, y)
                    
                    # ä¿å­˜scaleråˆ°æœ€ä½³æ¨¡å‹
                    search.best_estimator_._scaler = scaler
                    search.best_estimator_._use_scaling = True
                else:
                    search.fit(X, y)
                    search.best_estimator_._use_scaling = False
                
                score = search.best_score_
                elapsed_time = time.time() - start_time
                
                print(f"    âœ… {name}: RÂ²={score:.3f} | å‚æ•°: {search.best_params_} | æ—¶é—´: {elapsed_time:.1f}s")
                
                if score > best_score:
                    best_score = score
                    best_model = search.best_estimator_
                    best_name = name
                    best_params = search.best_params_
                    
            except Exception as e:
                print(f"    âŒ {name} å¤±è´¥: {e}")
                continue
        
        if best_model is None:
            # å¤‡ç”¨æ¨¡å‹
            print(f"    ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ¨¡å‹...")
            best_model = RandomForestRegressor(n_estimators=100, random_state=42)
            best_model.fit(X, y)
            best_model._use_scaling = False
            best_name = 'RandomForest_Backup'
            best_score = 0.1
            best_params = {}
        
        print(f"  ğŸ† æœ€ä½³æ¨¡å‹: {best_name} (RÂ²={best_score:.3f})")
        
        return best_model, {
            'model_name': best_name, 
            'r2': best_score, 
            'best_params': best_params
        }
    
    def train_all_models(self, feature_df):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print(f"\nğŸ¤– å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒ...")
        print(f"ğŸ”§ é…ç½®: {'éšæœºæœç´¢' if self.use_randomized_search else 'ç½‘æ ¼æœç´¢'} | CVæŠ˜æ•°: {self.cv_folds} | è¿­ä»£æ¬¡æ•°: {self.n_iter}")
        
        # ç‰¹å¾é€‰æ‹©
        feature_cols = self.select_optimal_features(feature_df)
        X = feature_df[feature_cols].fillna(0)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'customer_avg': feature_df['æ¥å®¢æ•°'].mean(),
            'beer_stats': {}
        }
        
        results = {}
        total_start_time = time.time()
        
        for i, beer in enumerate(self.beer_types, 1):
            target_col = f'{beer}(æœ¬)'
            if target_col not in feature_df.columns:
                print(f"âš ï¸ è·³è¿‡ {beer}: æœªæ‰¾åˆ°é”€é‡åˆ—")
                continue
            
            print(f"\n{'='*50}")
            print(f"ğŸº [{i}/{len(self.beer_types)}] è®­ç»ƒ {beer}")
            print(f"{'='*50}")
            
            y = feature_df[target_col]
            self.stats['beer_stats'][beer] = y.mean()
            
            try:
                model, performance = self.hyperparameter_optimization(beer, X, y)
                self.models[beer] = model
                self.best_params[beer] = performance['best_params']
                results[beer] = performance
                
            except Exception as e:
                print(f"âŒ {beer} è®­ç»ƒå¤±è´¥: {e}")
                results[beer] = {'error': str(e)}
        
        total_elapsed = time.time() - total_start_time
        print(f"\nâ±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_elapsed:.1f}ç§’ ({total_elapsed/60:.1f}åˆ†é’Ÿ)")
        
        return results
    
    def save_models(self, model_dir='trained_models_hyperopt'):
        """ä¿å­˜è¶…å‚æ•°ä¼˜åŒ–æ¨¡å‹"""
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        
        # ä¿å­˜æ¨¡å‹
        for beer, model in self.models.items():
            with open(f'{model_dir}/{beer}_model.pkl', 'wb') as f:
                pickle.dump(model, f)
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        scalers = {}
        for beer, model in self.models.items():
            if hasattr(model, '_scaler'):
                scalers[f'{beer}_scaler'] = model._scaler
        
        if scalers:
            with open(f'{model_dir}/scalers.pkl', 'wb') as f:
                pickle.dump(scalers, f)
        
        # ä¿å­˜å…ƒæ•°æ®ï¼ˆåŒ…å«æœ€ä½³å‚æ•°ï¼‰
        metadata = {
            'feature_names': self.feature_names,
            'beer_types': self.beer_types,
            'stats': self.stats,
            'best_params': self.best_params,
            'model_version': 'hyperopt_v1',
            'search_method': 'RandomizedSearch' if self.use_randomized_search else 'GridSearch',
            'cv_folds': self.cv_folds,
            'n_iter': self.n_iter
        }
        with open(f'{model_dir}/metadata.pkl', 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"âœ… è¶…å‚æ•°ä¼˜åŒ–æ¨¡å‹ä¿å­˜å®Œæˆ: {model_dir}")
    
    def run_complete_training(self, sales_file='sales.csv', weather_file='weather.csv'):
        """è¿è¡Œå®Œæ•´è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒæµç¨‹"""
        print("ğŸš€ è¶…å‚æ•°ä¼˜åŒ–å•¤é…’é¢„æµ‹æ¨¡å‹è®­ç»ƒ")
        print("="*60)
        
        try:
            # 1. æ•°æ®åŠ è½½
            df = self.load_and_prepare_data(sales_file, weather_file)
            
            # 2. ç‰¹å¾å·¥ç¨‹
            feature_df = self.create_improved_features(df)
            
            # 3. è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒ
            results = self.train_all_models(feature_df)
            
            # 4. ä¿å­˜æ¨¡å‹
            self.save_models()
            
            # 5. ç»“æœæ€»ç»“
            print("\n" + "="*60)
            print("ğŸ¯ è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒç»“æœæ€»ç»“")
            print("="*60)
            
            successful_models = 0
            total_r2 = 0
            
            for beer, result in results.items():
                if 'error' not in result:
                    r2 = result['r2']
                    model_name = result['model_name']
                    print(f"{beer}:")
                    print(f"  ğŸ† æœ€ä½³æ¨¡å‹: {model_name}")
                    print(f"  ğŸ“Š RÂ²: {r2:.3f}")
                    print(f"  âš™ï¸  æœ€ä½³å‚æ•°: {result['best_params']}")
                    print()
                    total_r2 += r2
                    successful_models += 1
                else:
                    print(f"{beer}: âŒ {result['error']}")
            
            if successful_models > 0:
                avg_r2 = total_r2 / successful_models
                print(f"ğŸ“Š å¹³å‡RÂ²: {avg_r2:.3f}")
                
                if avg_r2 > 0.7:
                    print("ğŸ‰ ä¼˜ç§€ï¼æ¨¡å‹ç²¾åº¦å¾ˆé«˜")
                elif avg_r2 > 0.5:
                    print("ğŸ‘ è‰¯å¥½ï¼æ¨¡å‹ç²¾åº¦å¯æ¥å—") 
                elif avg_r2 > 0.3:
                    print("âš ï¸ ä¸€èˆ¬ï¼Œä½†åº”è¯¥èƒ½æ”¹å–„é¢„æµ‹")
                else:
                    print("ğŸš¨ ç²¾åº¦è¾ƒä½ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–ç‰¹å¾")
            
            print(f"\nğŸ”§ è¶…å‚æ•°ä¼˜åŒ–æ”¹è¿›:")
            print(f"  âœ… è‡ªåŠ¨æœç´¢æœ€ä½³å‚æ•°ç»„åˆ")
            print(f"  âœ… ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯")
            print(f"  âœ… å¤šæ¨¡å‹è‡ªåŠ¨æ¯”è¾ƒé€‰æ‹©")
            print(f"  âœ… æ ‡å‡†åŒ–å¤„ç†çº¿æ€§æ¨¡å‹")
            print(f"  âœ… ä¿å­˜æœ€ä½³å‚æ•°é…ç½®")
            
            print(f"\nğŸ“ æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨: trained_models_hyperopt/")
            print(f"ğŸ”„ ä½¿ç”¨æ–¹æ³•: predictor = SimpleBeerPredictor(model_dir='trained_models_hyperopt')")
            
            return results
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

# ä½¿ç”¨ç¤ºä¾‹å’Œä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•°"""
    print("ğŸº è¶…å‚æ•°ä¼˜åŒ–å•¤é…’é¢„æµ‹æ¨¡å‹è®­ç»ƒå™¨")
    print("ä½¿ç”¨äº¤å‰éªŒè¯è‡ªåŠ¨è°ƒä¼˜ï¼Œæé«˜æ¨¡å‹ç²¾åº¦") 
    print("="*60)
    print("ğŸ“ è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶åœ¨å½“å‰ç›®å½•:")
    print("   - sales.csv")
    print("   - weather.csv")
    print("="*60)
    
    # é…ç½®é€‰é¡¹
    print("ğŸ”§ è®­ç»ƒé…ç½®é€‰é¡¹:")
    print("1. å¿«é€Ÿæ¨¡å¼: éšæœºæœç´¢ + 3æŠ˜CV + 50æ¬¡è¿­ä»£ (æ¨è)")
    print("2. ç²¾ç¡®æ¨¡å¼: ç½‘æ ¼æœç´¢ + 3æŠ˜CV (æ›´æ…¢ä½†æ›´ç²¾ç¡®)")
    print("3. æ·±åº¦æ¨¡å¼: éšæœºæœç´¢ + 5æŠ˜CV + 100æ¬¡è¿­ä»£ (æœ€æ…¢ä½†æœ€ç²¾ç¡®)")
    
    choice = input("\nè¯·é€‰æ‹©æ¨¡å¼ (1/2/3ï¼Œç›´æ¥å›è½¦é»˜è®¤é€‰æ‹©1): ").strip()
    
    if choice == '2':
        trainer = HyperOptBeerModelTrainer(
            use_randomized_search=False, 
            cv_folds=3, 
            n_iter=50
        )
        print("ğŸ¯ é€‰æ‹©: ç²¾ç¡®æ¨¡å¼ - ç½‘æ ¼æœç´¢")
    elif choice == '3':
        trainer = HyperOptBeerModelTrainer(
            use_randomized_search=True, 
            cv_folds=5, 
            n_iter=100
        )
        print("ğŸ¯ é€‰æ‹©: æ·±åº¦æ¨¡å¼ - 5æŠ˜CV + 100æ¬¡è¿­ä»£")
    else:
        trainer = HyperOptBeerModelTrainer(
            use_randomized_search=True, 
            cv_folds=3, 
            n_iter=50
        )
        print("ğŸ¯ é€‰æ‹©: å¿«é€Ÿæ¨¡å¼ - éšæœºæœç´¢ (æ¨è)")
    
    try:
        # è¿è¡Œå®Œæ•´è®­ç»ƒ
        results = trainer.run_complete_training('sales.csv', 'weather.csv')
        
        print("\nğŸ‰ è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒå®Œæˆï¼ä¸»è¦æˆæœ:")
        print("1. ğŸ¯ è‡ªåŠ¨æ‰¾åˆ°æ¯ä¸ªå•¤é…’ç±»å‹çš„æœ€ä½³æ¨¡å‹å’Œå‚æ•°")
        print("2. ğŸ“Š ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›")
        print("3. ğŸš€ å¤šç§ç®—æ³•å¯¹æ¯”: éšæœºæ£®æ—ã€æ¢¯åº¦æå‡ã€Extra Treesã€çº¿æ€§æ¨¡å‹")
        print("4. âš™ï¸  æœ€ä½³å‚æ•°é…ç½®å·²ä¿å­˜ï¼Œå¯é‡ç°ç»“æœ")
        print("5. ğŸ”§ æ ‡å‡†åŒ–å¤„ç†ç¡®ä¿çº¿æ€§æ¨¡å‹æ•ˆæœ")
        
        print("\nğŸ”„ ä½¿ç”¨æ–°çš„è¶…å‚æ•°ä¼˜åŒ–æ¨¡å‹:")
        print("```python")
        print("from predictor import SimpleBeerPredictor")
        print("predictor = SimpleBeerPredictor(model_dir='trained_models_hyperopt')")
        print("result = predictor.predict_date_sales('2025-06-21', customer_count=17)")
        print("print(result)")
        print("```")
        
        print("\nğŸ“ˆ æœŸæœ›æ”¹è¿›æ•ˆæœ:")
        print("- é¢„æµ‹ç²¾åº¦æå‡ 10-20%")
        print("- æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›")
        print("- å‡å°‘è¿‡æ‹Ÿåˆé£é™©")
        print("- è‡ªåŠ¨æ¨¡å‹é€‰æ‹©")
        
        return True
        
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("è¯·ç¡®ä¿ sales.csv å’Œ weather.csv åœ¨å½“å‰ç›®å½•")
        return False
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ¯ æ¨¡å‹ç²¾åº¦åº”è¯¥æœ‰æ˜¾è‘—æå‡ï¼")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")